{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbdeac20",
   "metadata": {},
   "source": "# cube"
  },
  {
   "cell_type": "code",
   "id": "ac5eb3c0",
   "metadata": {},
   "source": [
    "# Loading the dataset function\n",
    "from dataset_functions import *\n",
    "from PCA_functions import *"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6261cf1a",
   "metadata": {},
   "source": "## Data split"
  },
  {
   "cell_type": "code",
   "id": "0d069bd0",
   "metadata": {},
   "source": [
    "# Splitting the Dataset: 33% training - 5% Validation - 56% Testing\n",
    "# Training set is balanced\n",
    "training = [1,3,8,11,17,22,23,24,25,32,34,44,45,47,49,50,52,53]\n",
    "validation = [18, 37, 42] \n",
    "testing = [2, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 19, 20, 21, 26, 27, 28, 29, 30, 31, 33, 36, 38, 39, 40, 41, 43, 46, 48, 51]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f61203f",
   "metadata": {},
   "source": [
    "# Reading the data"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training PCA\n",
    "\n",
    "# Initialize empty lists to store training data\n",
    "Training_cube = []\n",
    "Training_masks = []\n",
    "\n",
    "# Iterate over training data\n",
    "for i in range(126):\n",
    "    \"\"\"\n",
    "    Load the PCA-transformed training data and corresponding segmentation masks from respective files\n",
    "    \"\"\"\n",
    "    # Define loading path\n",
    "    # saving_path =  \"PCBDataset/cube/train/\"     # e.g.,: /home/PCBvision/PCA/train/\n",
    "    saving_path =  \"D:/cube/train/\"     # e.g.,: /home/PCBvision/PCA/train/\n",
    "\n",
    "    # Construct the filename for the PCA-transformed cube\n",
    "    header_file = f\"{saving_path}{i}.hdr\"\n",
    "    # Construct file paths for PCA data and mask\n",
    "    data_file = header_file[:-4]\n",
    "\n",
    "    # Open the PCA data file and extract the PCA-transformed hyperspectral image\n",
    "    numpy_ndarr = envi.open(header_file, data_file)\n",
    "    c = spi.io.bipfile.BipFile.open_memmap(numpy_ndarr)\n",
    "    Training_cube.append(c)\n",
    "\n",
    "    # Open the mask file and load the segmentation mask\n",
    "    mask_file = f\"{saving_path}{i}.npy\"\n",
    "    m = np.load(mask_file, mmap_mode='r')\n",
    "    Training_masks.append(m)"
   ],
   "id": "29cffbc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training patch visualization\n",
    "i = 52\n",
    "plt.imshow(Training_cube[i][:,:,[21]])\n",
    "plt.axis('off')\n",
    "plt.figure()\n",
    "visualize(Training_masks[i])"
   ],
   "id": "8e9e39bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74867cf4",
   "metadata": {},
   "source": [
    "# Load validation patches and masks\n",
    "Validation_cube = []  # List to store validation patches\n",
    "Validation_masks = []  # List to store validation masks\n",
    "\n",
    "\n",
    "# Iterate over training data\n",
    "for i in range(3):\n",
    "\n",
    "\n",
    "    saving_path =  \"D:/cube/validation/\"     # e.g.,: /home/PCBvision/PCA/train/\n",
    "\n",
    "\n",
    "    header_file = f\"{saving_path}{i}.hdr\"\n",
    "    # Construct file paths for PCA data and mask\n",
    "    data_file = header_file[:-4]\n",
    "\n",
    "\n",
    "    # Open the header file and data file using the Envi library\n",
    "    numpy_ndarr = envi.open(header_file, data_file)\n",
    "    c = spi.io.bipfile.BipFile.open_memmap(numpy_ndarr)\n",
    "    # Append the loaded patch to the list of validation patches\n",
    "    Validation_cube.append(c)\n",
    "\n",
    "\n",
    "    mask_file = f\"{saving_path}{i}.npy\"\n",
    "    m = np.load(mask_file, mmap_mode='r')\n",
    "\n",
    "    Validation_masks.append(m)\n",
    "\n",
    "print(len(Validation_cube))\n",
    "print(len(Validation_masks))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e599fd0e",
   "metadata": {},
   "source": [
    "# Validation patch visualization\n",
    "i = 1\n",
    "plt.imshow(Validation_cube[i][:,:,(90,55,32)])\n",
    "plt.axis('off')\n",
    "plt.figure()\n",
    "visualize(Validation_masks[i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "07e8ecb9",
   "metadata": {},
   "source": [
    "# Testing patches loading\n",
    "Testing_cube = []  # List to store testing patches\n",
    "Testing_masks = []  # List to store testing masks\n",
    "\n",
    "# If using the monoseg annotations, don't forget to load them too !!\n",
    "\n",
    "# Iterate over 690 testing patches\n",
    "for i in range(30):\n",
    "\n",
    "    # Define loading path\n",
    "    # saving_path =  \"PCBDataset/cube/test/\"     # e.g.,: /home/PCBvision/PCA/train/\n",
    "    saving_path =  \"D:/cube/test/\"     # e.g.,: /home/PCBvision/PCA/train/\n",
    "\n",
    "    # Construct the filename for the PCA-transformed cube\n",
    "    header_file = f\"{saving_path}{i}.hdr\"\n",
    "    # Construct file paths for PCA data and mask\n",
    "    data_file = header_file[:-4]\n",
    "\n",
    "    # Open the PCA data file and extract the PCA-transformed hyperspectral image\n",
    "    numpy_ndarr = envi.open(header_file, data_file)\n",
    "    c = spi.io.bipfile.BipFile.open_memmap(numpy_ndarr)\n",
    "    Testing_cube.append(c)\n",
    "\n",
    "    # Open the mask file and load the segmentation mask\n",
    "    mask_file = f\"{saving_path}{i}.npy\"\n",
    "    m = np.load(mask_file, mmap_mode='r')\n",
    "    Testing_masks.append(m)\n",
    "\n",
    "print(len(Testing_cube))\n",
    "print(len(Testing_masks))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6f349f75",
   "metadata": {},
   "source": [
    "# Training - validation - testing sets check\n",
    "len(Training_cube), len(Training_masks), len(Validation_cube), len(Validation_masks), len(Testing_cube), len(Testing_masks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Resizing",
   "id": "753aced30602f215"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Training_cube, Training_masks2 = resize_hyperspectral_images(Training_cube,Training_masks,640)\n",
    "Training_cube[0].shape, Training_masks2[0].shape, Training_masks[0].shape"
   ],
   "id": "3f00f481c73ff934",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Validation_cube, Validation_masks2 = resize_hyperspectral_images(Validation_cube, Validation_masks,640)\n",
    "Validation_cube[0].shape, Validation_masks2[0].shape,Validation_masks[0].shape"
   ],
   "id": "ed56afc33dfe04f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "Testing_cube, Testing_masks2 = resize_hyperspectral_images(Testing_cube, Testing_masks,640)\n",
    "Testing_cube[0].shape, Testing_masks2[0].shape, Testing_masks[0].shape"
   ],
   "id": "429333793236d67d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c587c581",
   "metadata": {},
   "source": [
    "# Deep Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "id": "674f91aa",
   "metadata": {},
   "source": [
    "# Training - validation - testing sample size check\n",
    "Training_cube[0].shape, Training_masks2[0].shape, Validation_cube[0].shape, Validation_masks2[0].shape, Testing_cube[0].shape, Testing_masks2[0].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6c29cd3",
   "metadata": {},
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        # Initialize the dataset with the input images and masks\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset, which is the number of input images\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve the image and mask corresponding to the given index\n",
    "        image = self.images[index]\n",
    "        mask = self.masks[index]\n",
    "\n",
    "        # Convert the image and mask to torch tensors\n",
    "        # The `torch.from_numpy()` function converts the numpy arrays to tensors\n",
    "        # The `.float()` attribute converts the tensors to floating-point format\n",
    "        # The `torch.as_tensor()` function converts the mask to a long tensor\n",
    "        image = torch.from_numpy(image.copy()).float()\n",
    "        mask = torch.from_numpy(mask.copy()).float()\n",
    "        mask = torch.as_tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        return image, mask"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### learning with Various Convolutional Models",
   "id": "16b59f75a69e72b1"
  },
  {
   "cell_type": "code",
   "id": "7934e5f3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from models.Unet import UNET\n",
    "from models.Unet_Attention import AttU_Net\n",
    "from models.ResUnet import ResUnet\n",
    "\n",
    "from models.End2End_Unet import End2EndUNet\n",
    "from models.End2End_ResUnet import End2EndResUnet\n",
    "from models.End2End_AttUnet import End2EndAttUnet\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "device = set_gpu(0)\n",
    "\n",
    "# Set up the hyperparameters and configuration\n",
    "num_classes = 4  # Number of output classes\n",
    "batch_size = 4  # Batch size for training\n",
    "learning_rate = 0.00005  # Learning rate for the optimizer\n",
    "num_epochs = 100  # Number of training epochs\n",
    "patience = 20  # Number of epochs to wait for improvement before early stopping\n",
    "\n",
    "# Define class weights to handle class imbalance\n",
    "class_weights = [.1, .7, .95, .8]  # Weights for each class\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)  # Convert weights to tensors and move to device\n",
    "\n",
    "\n",
    "# Option 1: U-Net\n",
    "# model = UNET(\n",
    "#         in_channels = 214,\n",
    "#         out_channels = num_classes\n",
    "# )\n",
    "\n",
    "# Option 2: Attention U-Net\n",
    "# model = AttU_Net(\n",
    "#         img_ch=214,\n",
    "#         output_ch=num_classes\n",
    "# )\n",
    "\n",
    "# Option 3: ResU-Net\n",
    "# model = ResUnet(\n",
    "#         channel=214,\n",
    "#         out_channel=num_classes\n",
    "# )\n",
    "\n",
    "\n",
    "# Option 4: End2End Unet\n",
    "# model = End2EndUNet(\n",
    "#         spec_in = 214,\n",
    "#         spec_hidden = 128,\n",
    "#         spec_out = 3,\n",
    "#         num_classes = 4,\n",
    "#         unet_features = [64, 128, 256, 512]\n",
    "# )\n",
    "\n",
    "# # Option 5: End2End Attention Unet\n",
    "# model = End2EndAttUnet(\n",
    "#         spec_in = 214,\n",
    "#         spec_hidden = 128,\n",
    "#         spec_out = 3,\n",
    "#         num_classes = 4,\n",
    "#\n",
    "# )\n",
    "#\n",
    "# # Option 6: End2End ResUnet\n",
    "model = End2EndResUnet(\n",
    "        spec_in = 214,\n",
    "        spec_hidden = 128,\n",
    "        spec_out = 3,\n",
    "        num_classes = 4,\n",
    "        Resnet_features = [64, 128, 256, 512]\n",
    ")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define training, validation, and testing datasets\n",
    "train_dataset = CustomDataset(Training_cube, Training_masks2)\n",
    "val_dataset = CustomDataset(Validation_cube, Validation_masks2)\n",
    "test_dataset = CustomDataset(Testing_cube, Testing_masks2)\n",
    "\n",
    "# Create data loaders for each dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Specify the path to save the model weights\n",
    "path = 'End2End_ResUnet.pth'\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "# print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "# print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "#\n",
    "# # Print model input/output shapes for verification\n",
    "# dummy_input = torch.randn(1, 214, 64, 64).to(device)  # Example input shape\n",
    "# with torch.no_grad():\n",
    "#     dummy_output = model(dummy_input)\n",
    "#     print(f\"\\nInput shape: {dummy_input.shape}\")\n",
    "#     print(f\"Output shape: {dummy_output.shape}\")\n",
    "#\n",
    "# # Example of how to check intermediate spectral processing\n",
    "# print(f\"\\nSpectral processing: 214 channels -> {spectral_reduction_channels} channels\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "970aac0a",
   "metadata": {},
   "source": [
    "## Training phase"
   ]
  },
  {
   "cell_type": "code",
   "id": "16019937",
   "metadata": {},
   "source": [
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop initialization\n",
    "best_epoch = 0  # Stores the epoch with the best validation loss so far\n",
    "best_val_loss = float('inf')  # Stores the minimum validation loss so far\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0  # Initialize the training loss\n",
    "    \n",
    "    # Iterate over training batches\n",
    "    for images, masks in train_loader:\n",
    "        \n",
    "        # Zero gradients for optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move the data to GPU\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Remove the channel dimension from masks\n",
    "        masks = torch.squeeze(masks, dim=1)\n",
    "        # Convert masks to one-hot encodings and match output shape\n",
    "        masks = torch.nn.functional.one_hot(masks, num_classes)\n",
    "        \n",
    "        # Correct the dimension order: (batch, row, col , channel) --> (batch, channel, row, col)\n",
    "        masks = masks.permute(0, 3,1,2)  # (16, 640, 640, 4) --> (16, 4, 640, 640)\n",
    "        images = images.permute(0, 3,1,2)  # (16, 128, 640, 640) --> (16, 214, 640, 640)\n",
    "        \n",
    "        # Masks must be float\n",
    "        masks = masks.type(torch.FloatTensor).to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        # Accumulate training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Print the average training loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss / len(train_loader):.4f}\")\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0  # Initialize the validation loss\n",
    "    \n",
    "    # Disable gradient calculation (not needed for validation)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            # Move the data to GPU\n",
    "            images = images.to(device)\n",
    "            masks = masks.type(torch.LongTensor)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Remove the channel dimension from masks\n",
    "            masks = torch.squeeze(masks, dim=1)\n",
    "            \n",
    "            # Convert masks to one-hot encodings and match output shape\n",
    "            masks = torch.nn.functional.one_hot(masks, num_classes)\n",
    "            masks = masks.permute(0, 3, 1, 2)\n",
    "            # For some reason the type has to be confirmed to float before the needed step, otherwise an error was generated\n",
    "            masks = masks.type(torch.FloatTensor)\n",
    "\n",
    "            images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images).to(device)\n",
    "            \n",
    "            # For some reason moving to the device has to be confirmed, otherwise an error was generated\n",
    "            # Move masks to the same device as outputs\n",
    "            masks = masks.to(outputs.device)\n",
    "            criterion = criterion.to(device)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Print average validation loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    # Check for improvement in validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        # Update best validation loss and epoch\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "        # Reset early stopping counter\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Save model weights\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Best Epoch so far [{epoch+1}/{num_epochs}], Best Validation Loss so far: {val_loss / len(val_loader):.4f}\")\n",
    "    else:\n",
    "        # Increment early stopping counter\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Trigger early stopping if there are no improvements for `patience` epochs\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered. No improvement in {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Print best epoch and corresponding validation loss\n",
    "print(f\"Best Epoch: {best_epoch + 1}, Best Validation Loss: {best_val_loss / len(val_loader):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loss visualization\n",
   "id": "f1b888677325e27b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epoch Train/Validation Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "49a4ac149509c4f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"train\")\n",
    "print(train_losses)\n",
    "print(\"val\")\n",
    "print(val_losses)"
   ],
   "id": "231ad51eb374bebc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76424462",
   "metadata": {},
   "source": [
    "## Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "id": "a69d5966",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model in evaluation mode: disables certain training-related features\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predicted masks\n",
    "predicted_masks = []\n",
    "\n",
    "# Iterate over the test images and their ground truth masks\n",
    "for images, masks in test_loader:\n",
    "        \n",
    "    # Move the data to GPU\n",
    "    images = images.to(device)  # Transfer the image data to the specified device\n",
    "    masks = masks.type(torch.LongTensor)  # Convert the ground truth masks to LongTensor format\n",
    "    masks = masks.to(device)  # Transfer the ground truth masks to the specified device\n",
    "\n",
    "    # Remove the channel dimension from masks: Flatten the masks to remove the channel dimension\n",
    "    masks = torch.squeeze(masks, dim=1)\n",
    "            \n",
    "    # To make the masks have same shape and style as the output of the model\n",
    "    masks = torch.nn.functional.one_hot(masks, num_classes) # Convert the masks to one-hot encodings\n",
    "    masks = masks.permute(0, 3, 1, 2)  # Transpose the masks to match the expected output shape\n",
    "    images = images.permute(0, 3, 1, 2)  # Transpose the images to match the expected input shape\n",
    "    masks = masks.type(torch.FloatTensor)  # Convert the masks to FloatTensor format\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # Forward pass the images through the model to obtain the predictions\n",
    "        output = model(images)  # Forward pass the images through the model to obtain the predictions \n",
    "            \n",
    "    # Remove batch dimension\n",
    "    output = torch.squeeze(output, dim=0)  # (1, 4, 128, 128) --> (4, 128, 128)\n",
    "    output = torch.nn.functional.softmax(output, dim=0)\n",
    "    output = torch.argmax(output, dim=0)\n",
    "    \n",
    "    # Transfer the output to CPU memory and convert it to a numpy array\n",
    "    predicted_mask = output.cpu().numpy()  \n",
    "    \n",
    "    # Append the predicted mask to the list\n",
    "    predicted_masks.append(predicted_mask)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ab66610",
   "metadata": {},
   "source": [
    "# Size & Dimension Checks\n",
    "len(predicted_masks),len(Testing_masks), predicted_masks[0].shape, Testing_masks2[0].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d99d0950",
   "metadata": {},
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "flops, params = get_model_complexity_info(model, (214, 640, 640))\n",
    "print(flops)\n",
    "print(params)\n",
    "\n",
    "i = 0\n",
    "visualize(predicted_masks[i])\n",
    "# plt.axis('off')\n",
    "plt.imshow(Testing_cube[i][:,:,(88,57,29)],alpha = 0.5)\n",
    "plt.figure()\n",
    "visualize(Testing_masks2[i])\n",
    "plt.axis('off')\n",
    "# plt.imshow(Testing_cube[i][:,:,(88,57,29)],alpha = 0.5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60b73132",
   "metadata": {},
   "source": [
    "## Evaluation phase"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert the list to a numpy array\n",
    "predicted_masks2 = np.array(predicted_masks)\n",
    "\n",
    "# Caculate the segmentation metrics\n",
    "confusion_matrix_sum, true_positive_sum, true_negative_sum, false_positive_sum, false_negative_sum, precision, recall, f1_score, pixel_accuracy_per_class, pixel_accuracy, iou, dice_coefficient, kappa = evaluate_segmentation(Testing_masks2, predicted_masks2, num_classes)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_sum)\n",
    "print(\"Pixel Accuracy:\")\n",
    "print(pixel_accuracy)\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "print(\"F1 Score:\")\n",
    "print(f1_score)\n",
    "print(\"Intersection over Union (IoU):\")\n",
    "print(iou)\n",
    "print(\"Dice Coefficient:\")\n",
    "print(dice_coefficient)\n",
    "print(\"Kappa Coefficient:\")\n",
    "print(kappa)\n"
   ],
   "id": "8b1dd27fa53b8661",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
