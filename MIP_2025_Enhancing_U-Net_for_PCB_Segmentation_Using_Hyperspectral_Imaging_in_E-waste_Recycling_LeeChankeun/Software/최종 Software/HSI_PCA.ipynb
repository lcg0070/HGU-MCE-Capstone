{
 "cells": [
  {
   "cell_type": "code",
   "id": "f64209e8",
   "metadata": {},
   "source": [
    "# Loading the dataset function\n",
    "\n",
    "from dataset_functions import *\n",
    "from PCA_functions import *\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a21a116",
   "metadata": {},
   "source": [
    "# HSI manual dataset splitting: 33% training - 5% Validation - 56% Testing\n",
    "\n",
    "training = [1,3,8,11,17,22,23,24,25,32,34,44,45,47,49,50,52,53]\n",
    "validation = [18, 37, 42] \n",
    "testing = [2, 5, 6, 7, 9, 10, 12, 13, 14, 15, 16, 19, 20, 21, 26, 27, 28, 29, 30, 31, 33, 36, 38, 39, 40, 41, 43, 46, 48, 51]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a95fab79",
   "metadata": {},
   "source": [
    "# PCA Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "id": "e425876f",
   "metadata": {},
   "source": [
    "# Training PCA\n",
    "\n",
    "# Initialize empty lists to store training data\n",
    "Training_PCA = []\n",
    "Training_masks = []\n",
    "\n",
    "# Iterate over training data\n",
    "for i in range(126):\n",
    "    \"\"\"\n",
    "    Load the PCA-transformed training data and corresponding segmentation masks from respective files\n",
    "    \"\"\"\n",
    "    # Define loading path\n",
    "    saving_path =  \"PCBDataset/pca/train/\"     # e.g.,: /home/PCBvision/PCA/train/\n",
    "    \n",
    "    # Construct the filename for the PCA-transformed cube\n",
    "    header_file = f\"{saving_path}{i}.hdr\"\n",
    "    # Construct file paths for PCA data and mask\n",
    "    data_file = header_file[:-4]\n",
    "\n",
    "    # Open the PCA data file and extract the PCA-transformed hyperspectral image\n",
    "    numpy_ndarr = envi.open(header_file, data_file)\n",
    "    c = spi.io.bipfile.BipFile.open_memmap(numpy_ndarr)\n",
    "    Training_PCA.append(c)\n",
    "\n",
    "    # Open the mask file and load the segmentation mask\n",
    "    mask_file = f\"{saving_path}{i}.npy\"\n",
    "    m = np.load(mask_file, mmap_mode='r')\n",
    "    Training_masks.append(m)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05bd0320",
   "metadata": {},
   "source": [
    "# PCA training data visualization\n",
    "i = 52\n",
    "plt.imshow(Training_PCA[i])\n",
    "plt.show()\n",
    "plt.figure()\n",
    "visualize(Training_masks[i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e51932a3",
   "metadata": {},
   "source": [
    "# Validation PCA\n",
    "\n",
    "# Initialize empty lists to store training data\n",
    "Validation_PCA = []\n",
    "Validation_masks = []\n",
    "\n",
    "# Iterate over training data\n",
    "for i in range(3):\n",
    "    \"\"\"\n",
    "    Load the PCA-transformed training data and corresponding segmentation masks from respective files\n",
    "    \"\"\"\n",
    "    # Define loading path\n",
    "    saving_path =  \"PCBDataset/pca/validation/\"     # e.g.,: /home/PCBvision/PCA/train/\n",
    "    \n",
    "    # Construct the filename for the PCA-transformed cube\n",
    "    header_file = f\"{saving_path}{i}.hdr\"\n",
    "    # Construct file paths for PCA data and mask\n",
    "    data_file = header_file[:-4]\n",
    "\n",
    "    # Open the PCA data file and extract the PCA-transformed hyperspectral image\n",
    "    numpy_ndarr = envi.open(header_file, data_file)\n",
    "    c = spi.io.bipfile.BipFile.open_memmap(numpy_ndarr)\n",
    "    Validation_PCA.append(c)\n",
    "\n",
    "    # Open the mask file and load the segmentation mask\n",
    "    mask_file = f\"{saving_path}{i}.npy\"\n",
    "    m = np.load(mask_file, mmap_mode='r')\n",
    "    Validation_masks.append(m)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cb86691",
   "metadata": {},
   "source": [
    "len(Validation_masks), len(Validation_PCA)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "646cfe51",
   "metadata": {},
   "source": [
    "# PCA validation data visualization\n",
    "i = 1\n",
    "plt.imshow(Validation_PCA[i])\n",
    "plt.figure()\n",
    "visualize(Validation_masks[i])\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ac93ce0",
   "metadata": {},
   "source": [
    "# Validation PCA\n",
    "\n",
    "# Initialize empty lists to store training data\n",
    "Testing_PCA = []\n",
    "Testing_masks = []\n",
    "\n",
    "# Iterate over training data\n",
    "for i in range(30):\n",
    "    \"\"\"\n",
    "    Load the PCA-transformed training data and corresponding segmentation masks from respective files\n",
    "    \"\"\"\n",
    "    # Define loading path\n",
    "    saving_path =  \"PCBDataset/pca/test/\"     # e.g.,: /home/PCBvision/PCA/train/\n",
    "    \n",
    "    # Construct the filename for the PCA-transformed cube\n",
    "    header_file = f\"{saving_path}{i}.hdr\"\n",
    "    # Construct file paths for PCA data and mask\n",
    "    data_file = header_file[:-4]\n",
    "\n",
    "    # Open the PCA data file and extract the PCA-transformed hyperspectral image\n",
    "    numpy_ndarr = envi.open(header_file, data_file)\n",
    "    c = spi.io.bipfile.BipFile.open_memmap(numpy_ndarr)\n",
    "    Testing_PCA.append(c)\n",
    "\n",
    "    # Open the mask file and load the segmentation mask\n",
    "    mask_file = f\"{saving_path}{i}.npy\"\n",
    "    m = np.load(mask_file, mmap_mode='r')\n",
    "    Testing_masks.append(m)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0fe84c0",
   "metadata": {},
   "source": [
    "len(Testing_PCA), len(Testing_masks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "beef85cc",
   "metadata": {},
   "source": [
    "# PCA test data visualization\n",
    "i = 1\n",
    "plt.imshow(Testing_PCA[i])\n",
    "plt.figure()\n",
    "visualize(Testing_masks[i])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66dacbcf",
   "metadata": {},
   "source": [
    "len(Training_PCA), len(Training_masks), len(Testing_PCA), len(Testing_masks),  len(Validation_PCA), len(Validation_masks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b968ecb0",
   "metadata": {},
   "source": [
    "# Resizing"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3894b00",
   "metadata": {},
   "source": [
    "Training_PCA, Training_masks2 = resize_hyperspectral_images(Training_PCA,Training_masks,640)\n",
    "Training_PCA[0].shape, Training_masks2[0].shape, Training_masks[0].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75a4634a",
   "metadata": {},
   "source": [
    "Validation_PCA, Validation_masks2 = resize_hyperspectral_images(Validation_PCA, Validation_masks,640)\n",
    "Validation_PCA[0].shape, Validation_masks2[0].shape,Validation_masks[0].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ef0b38f",
   "metadata": {},
   "source": [
    "Testing_PCA, Testing_masks2 = resize_hyperspectral_images(Testing_PCA, Testing_masks,640)\n",
    "Testing_PCA[0].shape, Testing_masks2[0].shape, Testing_masks[0].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f97e498",
   "metadata": {},
   "source": [
    "# Visualization after resizing \n",
    "i = 2\n",
    "plt.imshow(Validation_PCA[i][:,:,:3])\n",
    "plt.figure()\n",
    "visualize(Validation_masks2[i])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f991856",
   "metadata": {},
   "source": [
    "# Deep Learning Part"
   ]
  },
  {
   "cell_type": "code",
   "id": "96b313d1",
   "metadata": {},
   "source": [
    "# data Size check after resizing\n",
    "Training_PCA[0].shape, Training_masks2[0].shape, Validation_PCA[0].shape, Validation_masks2[0].shape, Testing_PCA[0].shape, Testing_masks2[0].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b14fdd5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        # Initialize the dataset with the input images and masks\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the length of the dataset, which is the number of input images\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve the image and mask corresponding to the given index\n",
    "        image = self.images[index]\n",
    "        mask = self.masks[index]\n",
    "\n",
    "        # Convert the image and mask to torch tensors\n",
    "        # The `torch.from_numpy()` function converts the numpy arrays to tensors\n",
    "        # The `.float()` attribute converts the tensors to floating-point format\n",
    "        # The `torch.as_tensor()` function converts the mask to a long tensor\n",
    "        image = torch.from_numpy(image.copy()).float()\n",
    "        mask = torch.from_numpy(mask.copy()).float()\n",
    "        mask = torch.as_tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        return image, mask"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72aadbd2",
   "metadata": {},
   "source": [
    "from models.Unet import UNET\n",
    "from models.Unet_Attention import AttU_Net\n",
    "from models.ResUnet import ResUnet\n",
    "\n",
    "device = set_gpu(0)\n",
    "\n",
    "# Set up the hyperparameters and configuration\n",
    "num_classes = 4  # Number of output classes\n",
    "batch_size = 8  # Batch size for training\n",
    "learning_rate = 0.00005  # Learning rate for the optimizer\n",
    "num_epochs = 100  # Number of training epochs\n",
    "patience = 20  # Number of epochs to wait for improvement before early stopping\n",
    "\n",
    "# Define class weights to handle class imbalance\n",
    "class_weights = [.1, .7, .95, .8]  # Weights for each class\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)  # Convert weights to tensors and move to device\n",
    "\n",
    "#  Call the DL model\n",
    "# model = UNET(in_channels = 3, out_channels = num_classes) # Specify input (input = Principal Component) and output channels\n",
    "# model = AttU_Net(img_ch=3,output_ch=num_classes)\n",
    "model = ResUnet(channel=3,out_channel=num_classes)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,reduction='mean')  # Use the class weights in the loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Use Adam optimizer with specified learning rate\n",
    "\n",
    "# Define training, validation, and testing sets\n",
    "train_dataset = CustomDataset(Training_PCA, Training_masks2)  # Create training dataset from training patches and masks\n",
    "val_dataset = CustomDataset(Validation_PCA, Validation_masks2)  # Create validation dataset from validation patches and masks\n",
    "test_dataset = CustomDataset(Testing_PCA, Testing_masks2)  # Create testing dataset from testing patches and masks\n",
    "\n",
    "# Create data loaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=True)  # Configure training data loader\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, num_workers=0, pin_memory=True)  # Configure validation data loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, pin_memory=True)  # Configure testing data loader\n",
    "\n",
    "# Specify the path to the pre-trained weights file: Change according to the chosen model !!\n",
    "path = 'General_PCA_ResUnet.pth'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e892f3f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f8eae0b5",
   "metadata": {},
   "source": [
    "## Training phase"
   ]
  },
  {
   "cell_type": "code",
   "id": "5128c81c",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training loop initialization\n",
    "best_epoch = 0  # Stores the epoch with the best validation loss so far\n",
    "best_val_loss = float('inf')  # Stores the minimum validation loss so far\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0.0  # Initialize the training loss\n",
    "\n",
    "    # Iterate over the training batches\n",
    "    for images, masks in train_loader:\n",
    "        \n",
    "        # Zero gradients for optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move the data to GPU\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        # Remove the channel dimension from masks\n",
    "        masks = torch.squeeze(masks, dim=1)\n",
    "        # Convert masks to one-hot encodings and match output shape\n",
    "        masks = torch.nn.functional.one_hot(masks, num_classes)\n",
    "        \n",
    "        # Correct the dimension order: (batch, row, col , channel) --> (batch, channel, row, col)\n",
    "        masks = masks.permute(0, 3,1,2)  # (8, 640, 640, 4) --> (8, 4, 640, 640)\n",
    "        images = images.permute(0, 3,1,2)  # (8, 640, 640, 3) --> (8, 3, 640, 640)\n",
    "        \n",
    "        # Masks must be float\n",
    "        masks = masks.type(torch.FloatTensor).to(device)\n",
    "    \n",
    "        # Forward pass\n",
    "        outputs = model(images).to(device)\n",
    "        loss = criterion(outputs, masks)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        loss.backward()  # Compute gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        # Accumulate training loss\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Print the average training loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss / len(train_loader):.4f}\")\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0  # Initialize the validation loss\n",
    "    \n",
    "    # Disable gradient calculation (not needed for validation)\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            # Move the data to GPU\n",
    "            images = images.to(device)\n",
    "            masks = masks.type(torch.LongTensor)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            # Remove the channel dimension from masks\n",
    "            masks = torch.squeeze(masks, dim=1)\n",
    "            \n",
    "            # Convert masks to one-hot encodings and match output shape\n",
    "            masks = torch.nn.functional.one_hot(masks, num_classes)\n",
    "            masks = masks.permute(0, 3, 1, 2)\n",
    "            # For some reason the type has to be confirmed to float before the needed step, otherwise an error was generated\n",
    "            masks = masks.type(torch.FloatTensor)\n",
    "\n",
    "            images = images.permute(0, 3, 1, 2)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images).to(device)\n",
    "            \n",
    "            # For some reason moving to the device has to be confirmed, otherwise an error was generated\n",
    "            # Move masks to the same device as outputs\n",
    "            masks = masks.to(outputs.device)\n",
    "            criterion = criterion.to(device)\n",
    "            \n",
    "            # Calculate validation loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Print average validation loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss / len(val_loader):.4f}\")\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "    # Check for improvement in validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        # Update best validation loss and epoch\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch\n",
    "\n",
    "        # Reset early stopping counter\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        # Save model weights\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Best Epoch so far [{epoch+1}/{num_epochs}], Best Validation Loss so far: {val_loss / len(val_loader):.4f}\")\n",
    "    else:\n",
    "        # Increment early stopping counter\n",
    "        epochs_without_improvement += 1\n",
    "    \n",
    "    # Trigger early stopping if there are no improvements for `patience` epochs\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f\"Early stopping triggered. No improvement in {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Print best epoch and corresponding validation loss\n",
    "print(f\"Best Epoch: {best_epoch + 1}, Best Validation Loss: {best_val_loss / len(val_loader):.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "loss visualization",
   "id": "7d6fef8b54425812"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epochë³„ Train/Validation Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d284c71c0479dfef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"train\")\n",
    "print(train_losses)\n",
    "print(\"val\")\n",
    "print(val_losses)"
   ],
   "id": "6104bd65eff5ac78",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2bf21906",
   "metadata": {},
   "source": [
    "## Testing phase"
   ]
  },
  {
   "cell_type": "code",
   "id": "8dcfa1e2",
   "metadata": {},
   "source": [
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load(path))\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize lists to store predicted masks\n",
    "predicted_masks = []\n",
    "\n",
    "# Iterate over the test images and their ground truth masks\n",
    "for images, masks in test_loader:\n",
    "    \n",
    "    # Move the data to GPU\n",
    "    images = images.to(device)  # Transfer the image data to the specified device\n",
    "    masks = masks.type(torch.LongTensor)  # Convert the ground truth masks to LongTensor format\n",
    "    masks = masks.to(device)  # Transfer the ground truth masks to the specified device\n",
    "\n",
    "    # Remove the channel dimension from masks: Flatten the masks to remove the channel dimension\n",
    "    masks = torch.squeeze(masks, dim=1)\n",
    "            \n",
    "    # To make the masks have same shape and style as the output of the model\n",
    "    masks = torch.nn.functional.one_hot(masks, num_classes)  # Convert the masks to one-hot encodings\n",
    "    masks = masks.permute(0, 3, 1, 2)  # Transpose the masks to match the expected output shape\n",
    "    images = images.permute(0, 3, 1, 2)  # Transpose the images to match the expected input shape\n",
    "    masks = masks.type(torch.FloatTensor)  # Convert the masks to FloatTensor format\n",
    "    \n",
    "    # Perform inference\n",
    "    with torch.no_grad():  # Forward pass the images through the model to obtain the predictions\n",
    "        output = model(images)  # Forward pass the images through the model to obtain the predictions \n",
    "\n",
    "    # Remove batch dimension\n",
    "    output = torch.squeeze(output, dim=0)  # (1, 4, 640, 640) --> (4, 640, 640)\n",
    "    output = torch.nn.functional.softmax(output, dim=0)\n",
    "    output = torch.argmax(output, dim=0)\n",
    "    \n",
    "    # Transfer the output to CPU memory and convert it to a numpy array\n",
    "    predicted_mask = output.cpu().numpy()\n",
    "\n",
    "    # Append the predicted mask to the list\n",
    "    predicted_masks.append(predicted_mask)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2c0ad28",
   "metadata": {},
   "source": [
    "# Size & Dimension Checks\n",
    "len(predicted_masks),len(Testing_masks2), predicted_masks[0].shape, Testing_masks2[0].shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "99fc324e",
   "metadata": {},
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "flops, params = get_model_complexity_info(model, (3, 640, 640))\n",
    "print(flops)\n",
    "print(params)\n",
    "\n",
    "visualize(predicted_masks[0])\n",
    "plt.axis('off')\n",
    "plt.figure()\n",
    "visualize(Testing_masks2[0])\n",
    "plt.show()\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "77762353",
   "metadata": {},
   "source": [
    "## Evaluation phase"
   ]
  },
  {
   "cell_type": "code",
   "id": "f7fff5dd",
   "metadata": {},
   "source": [
    "# Resize predicted masks to original shape\n",
    "predicted_masks2 = []\n",
    "\n",
    "for i, m in enumerate(predicted_masks):\n",
    "    \"\"\"\n",
    "    Resize each predicted mask to the original size of its corresponding ground truth mask.\n",
    "\n",
    "    Parameters:\n",
    "        i (int): Index of the current mask\n",
    "        m (numpy.ndarray): Predicted mask\n",
    "        Testing_masks[i] (numpy.ndarray): Corresponding ground truth mask\n",
    "    \"\"\"\n",
    "\n",
    "    # Resize the predicted mask using the dimensions of the ground truth mask\n",
    "    predicted_masks2.append(resize_segmentation_masks(m, Testing_masks[i].shape))\n",
    "\n",
    "print(predicted_masks2[0].shape, Testing_masks[0].shape)  # Print the shape of the resized and original masks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "551c3080",
   "metadata": {},
   "source": [
    "# Convert the list to a numpy array\n",
    "predicted_masks = np.array(predicted_masks)\n",
    "\n",
    "# Call the evaluate_segmentation function\n",
    "confusion_matrix_sum, true_positive_sum, true_negative_sum, false_positive_sum, false_negative_sum, precision, recall, f1_score, pixel_accuracy_per_class, pixel_accuracy, iou, dice_coefficient, kappa = evaluate_segmentation(Testing_masks2, predicted_masks, num_classes)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix_sum)\n",
    "print(\"Pixel Accuracy:\")\n",
    "print(pixel_accuracy)\n",
    "print(\"Precision:\")\n",
    "print(precision)\n",
    "print(\"Recall:\")\n",
    "print(recall)\n",
    "print(\"F1 Score:\")\n",
    "print(f1_score)\n",
    "print(\"Intersection over Union (IoU):\")\n",
    "print(iou)\n",
    "print(\"Dice Coefficient:\")\n",
    "print(dice_coefficient)\n",
    "print(\"Kappa Coefficient:\")\n",
    "print(kappa)\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
